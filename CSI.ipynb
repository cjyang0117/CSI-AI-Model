{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#tf.compat.v1.set_random_seed(2)\n",
    "#np.random.seed(2)\n",
    "\n",
    "def add_layer(inputs, input_size, output_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random_uniform([input_size, output_size], minval=-0.05, maxval=0.05))\n",
    "    threshold = tf.Variable(tf.zeros([1, output_size]) + 1)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + threshold\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs, Weights, threshold\n",
    "\n",
    "def nStage(x_data, y_data, n, inputSize):\n",
    "    tmp = np.full((n, inputSize), 0.00)\n",
    "    tmp2 = np.full((n, 1), 0.00)           \n",
    "    for i in range(n):\n",
    "        tmp[i] = x_data[i]\n",
    "        tmp2[i] = y_data[i]\n",
    "        \n",
    "    a_data = []\n",
    "    b_data = []\n",
    "    for i in range(n):\n",
    "        if tmp2[i] > 0:\n",
    "            a_data.append(i)\n",
    "        else:\n",
    "            b_data.append(i)\n",
    "    return tmp, tmp2, a_data, b_data \n",
    "\n",
    "def LTS_sort(error, x_data, y_data, n):\n",
    "    for i in range(1, n):\n",
    "        tmp = np.array(error[i])\n",
    "        tmp2 = np.array(x_data[i])\n",
    "        tmp3 = np.array(y_data[i])\n",
    "        j = i\n",
    "        while j > 0 and tmp < error[j - 1]:\n",
    "            error[j] = error[j - 1]\n",
    "            x_data[j] = x_data[j - 1]\n",
    "            y_data[j] = y_data[j - 1]\n",
    "            j-=1   \n",
    "        error[j] = tmp\n",
    "        x_data[j] = tmp2\n",
    "        y_data[j] = tmp3\n",
    "\n",
    "    if y_data[0] == y_data[1]:\n",
    "        for i in range(n - 2):\n",
    "            if y_data[0] != y_data[i + 2]:\n",
    "                tmp = np.array(x_data[1])\n",
    "                tmp2 = np.array(y_data[1])\n",
    "                x_data[1] = x_data[i + 2]\n",
    "                y_data[1] = y_data[i + 2]\n",
    "                x_data[i + 2] = tmp\n",
    "                y_data[i + 2] = tmp2\n",
    "                break\n",
    "    return x_data, y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60\n",
    "inputSize = 22\n",
    "fp = open(\"SPECT Heart train x_data.txt\", \"r\", encoding = \"utf-8\")\n",
    "x_data = np.full((N, inputSize), 0.00)\n",
    "\n",
    "for i in range(N):\n",
    "    line = fp.readline()\n",
    "    for j in range(inputSize):\n",
    "        x_data[i][j] = np.asarray(line.split(), dtype=np.float64)[j]\n",
    "fp.close()\n",
    "\n",
    "fp = open(\"SPECT Heart train y_data.txt\", \"r\")\n",
    "line = fp.readline()\n",
    "y_data = []\n",
    "\n",
    "while line:\n",
    "    y_data.append(line.strip())\n",
    "    line = fp.readline()\n",
    "y_data = np.asarray(y_data, dtype=np.int32)\n",
    "y_data = np.reshape(y_data, (N, -1))\n",
    "from sklearn.utils import shuffle\n",
    "x_data, y_data = shuffle(x_data, y_data, random_state=0)\n",
    "\n",
    "a = -1\n",
    "b = 1\n",
    "\n",
    "for i in range(N): \n",
    "    if y_data[i] > 0:\n",
    "        a_index = i\n",
    "        break\n",
    "for i in range(N): \n",
    "    if y_data[i] < 0:\n",
    "        b_index = i\n",
    "        break\n",
    "    \n",
    "nStage_x_data = np.full((2, inputSize), 0.00)\n",
    "nStage_x_data[0] = x_data[a_index]\n",
    "nStage_x_data[1] = x_data[b_index]\n",
    "nStage_y_data = np.full((2, 1), 0.00)\n",
    "nStage_y_data[0] = y_data[a_index]\n",
    "nStage_y_data[1] = y_data[b_index]\n",
    "a_data = [0]\n",
    "b_data = [1]\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None, inputSize])\n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenLayer, weights_h, threshold_h = add_layer(xs, inputSize, 1, tf.nn.relu)\n",
    "outputLayer, weights_o, threshold_o = add_layer(hiddenLayer, 1, 1)\n",
    "hiddenNodesNum = 1\n",
    "\n",
    "error = tf.reduce_mean(tf.reduce_sum(tf.square(ys - outputLayer), reduction_indices=[1]))\n",
    "#regularizers = tf.nn.l2_loss(weights_h) + tf.nn.l2_loss(weights_o) + tf.nn.l2_loss(threshold_h) + tf.nn.l2_loss(threshold_o)\n",
    "regularizers = tf.math.multiply((0.001/(hiddenNodesNum + 1 + hiddenNodesNum * (inputSize + 1))), tf.reduce_sum(tf.square(weights_o))+tf.reduce_sum(tf.square(weights_h))+tf.reduce_sum(tf.square(threshold_h))+tf.reduce_sum(tf.square(threshold_o)))\n",
    "error = error + regularizers\n",
    "error2 = tf.square(ys - outputLayer)\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(error)\n",
    "\n",
    "init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: \n",
      "a:  [0.5957364]\n",
      "b:  [0.5588237]\n",
      "stage 3 / 60 :\n",
      "   learning_times: 0\n",
      "stage 4 / 60 :\n",
      "   learning_times: 0\n",
      "stage 5 / 60 :\n",
      "   learning_times: 0\n",
      "stage 6 / 60 :\n",
      "   learning_times: 0\n",
      "stage 7 / 60 :\n",
      "   learning_times: 0\n",
      "stage 8 / 60 :\n",
      "   learning_times: 0\n",
      "stage 9 / 60 :\n",
      "   learning_times: 0\n",
      "stage 10 / 60 :\n",
      "   learning_times: 0\n",
      "stage 11 / 60 :\n",
      "   learning_times: 0\n",
      "stage 12 / 60 :\n",
      "   learning_times: 0\n",
      "stage 13 / 60 :\n",
      "   learning_times: 0\n",
      "stage 14 / 60 :\n",
      "   learning_times: 0\n",
      "stage 15 / 60 :\n",
      "   learning_times: 0\n",
      "stage 16 / 60 :\n",
      "   learning_times: 0\n",
      "stage 17 / 60 :\n",
      "   learning_times: 0\n",
      "stage 18 / 60 :\n",
      "   learning_times: 0\n",
      "stage 19 / 60 :\n",
      "   learning_times: 0\n",
      "stage 20 / 60 :\n",
      "   learning_times: 0\n",
      "stage 21 / 60 :\n",
      "   learning_times: 0\n",
      "stage 22 / 60 :\n",
      "   learning_times: 0\n",
      "stage 23 / 60 :\n",
      "   learning_times: 0\n",
      "stage 24 / 60 :\n",
      "   learning_times: 0\n",
      "stage 25 / 60 :\n",
      "   learning_times: 0\n",
      "stage 26 / 60 :\n",
      "   learning_times: 0\n",
      "stage 27 / 60 :\n",
      "   learning_times: 0\n",
      "stage 28 / 60 :\n",
      "   learning_times: 0\n",
      "stage 29 / 60 :\n",
      "   learning_times: 0\n",
      "stage 30 / 60 :\n",
      "   learning_times: 0\n",
      "stage 31 / 60 :\n",
      "   learning_times: 0\n",
      "stage 32 / 60 :\n",
      "   learning_times: 2\n",
      "stage 33 / 60 :\n",
      "   learning_times: 2\n",
      "stage 34 / 60 :\n",
      "   learning_times: 4\n",
      "stage 35 / 60 :\n",
      "   learning_times: 3\n",
      "stage 36 / 60 :\n",
      "   learning_times: 0\n",
      "stage 37 / 60 :\n",
      "   learning_times: 4\n",
      "stage 38 / 60 :\n",
      "   learning_times: 4\n",
      "stage 39 / 60 :\n",
      "   learning_times: 8\n",
      "stage 40 / 60 :\n",
      "   learning_times: 5\n",
      "stage 41 / 60 :\n",
      "   learning_times: 1\n",
      "stage 42 / 60 :\n",
      "   learning_times: 2\n",
      "stage 43 / 60 :\n",
      "   loss:  0.22684364\n",
      "   loss:  0.18964408\n",
      "   loss:  0.17108905\n",
      "   learning_times: 33\n",
      "stage 44 / 60 :\n",
      "   learning_times: 0\n",
      "stage 45 / 60 :\n",
      "   learning_times: 0\n",
      "stage 46 / 60 :\n",
      "   learning_times: 0\n",
      "stage 47 / 60 :\n",
      "   learning_times: 0\n",
      "stage 48 / 60 :\n",
      "   learning_times: 0\n",
      "stage 49 / 60 :\n",
      "   learning_times: 5\n",
      "stage 50 / 60 :\n",
      "   learning_times: 0\n",
      "stage 51 / 60 :\n",
      "   learning_times: 0\n",
      "stage 52 / 60 :\n",
      "   learning_times: 0\n",
      "stage 53 / 60 :\n",
      "   learning_times: 1\n",
      "stage 54 / 60 :\n",
      "   learning_times: 3\n",
      "stage 55 / 60 :\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-499e3eebb78c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_weights_o\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold_h\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_threshold_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthreshold_o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_threshold_o\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_pre2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_pre2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_pre2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_pre2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_pre2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1341\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_pre2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Step1:　Two cases + Learning goal + one hidden node\n",
    "lr = 0.1\n",
    "while a <= b:\n",
    "    sess.run(train, feed_dict={xs: nStage_x_data, ys: nStage_y_data, learning_rate: lr})\n",
    "    pred_ = sess.run(outputLayer, feed_dict={xs: nStage_x_data, ys: nStage_y_data, learning_rate: lr})\n",
    "    a = min(pred_[a_data])\n",
    "    b = max(pred_[b_data])\n",
    "print(\"Step1: \")\n",
    "print(\"a: \", a)\n",
    "print(\"b: \", b)\n",
    "    \n",
    "learning_times = 1\n",
    "n = 3\n",
    "ud = False\n",
    "\n",
    "#Step2: n > N?\n",
    "while n <= N:\n",
    "    print(\"stage\", n, \"/\", N, \":\")\n",
    "    #Step3: LTS\n",
    "    error_ = sess.run(error2, feed_dict={xs: x_data, ys: y_data})\n",
    "    x_data, y_data = LTS_sort(error_, x_data, y_data, N)\n",
    "    nStage_x_data, nStage_y_data, a_data, b_data = nStage(x_data, y_data, n, inputSize)\n",
    "    \n",
    "    pred_, temp_loss, temp_weights_h, temp_weights_o, temp_threshold_h, temp_threshold_o = sess.run([outputLayer, error, weights_h, weights_o, threshold_h, threshold_o], feed_dict={xs: nStage_x_data, ys: nStage_y_data})\n",
    "    a = min(pred_[a_data])\n",
    "    b = max(pred_[b_data])\n",
    "    lr = 0.1\n",
    "    learning_times = 0\n",
    "    \n",
    "    #Step4: Learning goal?\n",
    "    while a <= b:\n",
    "        #Step5: Save w\n",
    "        n1Stage_a = a\n",
    "        n1Stage_b = b\n",
    "        n1Stage_nCase = pred_[n - 1]\n",
    "        n1Stage_weights_h, n1Stage_weights_o, n1Stage_threshold_h, n1Stage_threshold_o = sess.run([weights_h, weights_o, threshold_h, threshold_o])\n",
    "        \n",
    "        #Step6: Weight-tuning + Cramming\n",
    "        sess.run(train, feed_dict={xs: nStage_x_data, ys: nStage_y_data, learning_rate: lr})\n",
    "        pred_, loss = sess.run([outputLayer, error], feed_dict={xs: nStage_x_data, ys: nStage_y_data})\n",
    "        if loss < temp_loss:\n",
    "            lr = lr * 1.2\n",
    "            a = min(pred_[a_data])\n",
    "            b = max(pred_[b_data])\n",
    "            temp_weights_h, temp_weights_o, temp_threshold_h, temp_threshold_o = sess.run([weights_h, weights_o, threshold_h, threshold_o])\n",
    "            temp_loss = loss\n",
    "            learning_times += 1\n",
    "            if learning_times % 10 == 0:\n",
    "                print(\"   loss: \", loss)\n",
    "        else:\n",
    "            if  lr > 0.05:      \n",
    "                sess.run(weights_h.assign(temp_weights_h))\n",
    "                sess.run(weights_o.assign(temp_weights_o))\n",
    "                sess.run(threshold_h.assign(temp_threshold_h))\n",
    "                sess.run(threshold_o.assign(temp_threshold_o))\n",
    "                lr = lr * 0.7\n",
    "            else:\n",
    "                print(\"   undesired attractor\")\n",
    "                #新增 hidden nodes 並將新增的 weights 權重設為0，新神經網路的output必須與原砷經網路相同\n",
    "                #error、init 需重新指派\n",
    "                hiddenNodesNum += 1                 \n",
    "                hiddenLayer, weights_h, threshold_h = add_layer(xs, inputSize, hiddenNodesNum, tf.nn.relu)\n",
    "                outputLayer, weights_o, threshold_o = add_layer(hiddenLayer, hiddenNodesNum, 1)\n",
    "                error = tf.reduce_mean(tf.reduce_sum(tf.square(ys - outputLayer), reduction_indices=[1]))\n",
    "                #regularizers = tf.nn.l2_loss(weights_h) + tf.nn.l2_loss(weights_o) + tf.nn.l2_loss(threshold_h) + tf.nn.l2_loss(threshold_o)\n",
    "                regularizers = tf.math.multiply((0.001/(hiddenNodesNum + 1 + hiddenNodesNum * (inputSize + 1))), tf.reduce_sum(tf.square(weights_o))+tf.reduce_sum(tf.square(weights_h))+tf.reduce_sum(tf.square(threshold_h))+tf.reduce_sum(tf.square(threshold_o)))\n",
    "                error = error + regularizers\n",
    "                error2 = tf.square(ys - outputLayer)\n",
    "                train = tf.train.GradientDescentOptimizer(learning_rate).minimize(error)\n",
    "\n",
    "                init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "                sess = tf.Session()\n",
    "                sess.run(init)\n",
    "\n",
    "                J = 1.1 #滔\n",
    "                new_weights_h = np.hstack([n1Stage_weights_h, np.vstack([J * nStage_x_data[n - 1][0], J * nStage_x_data[n - 1][1], J * nStage_x_data[n - 1][2], J * nStage_x_data[n - 1][3], J * nStage_x_data[n - 1][4], J * nStage_x_data[n - 1][5], J * nStage_x_data[n - 1][6], J * nStage_x_data[n - 1][7], J * nStage_x_data[n - 1][8], J * nStage_x_data[n - 1][9], J * nStage_x_data[n - 1][10], J * nStage_x_data[n - 1][11], J * nStage_x_data[n - 1][12], J * nStage_x_data[n - 1][13], J * nStage_x_data[n - 1][14], J * nStage_x_data[n - 1][15], J * nStage_x_data[n - 1][16], J * nStage_x_data[n - 1][17], J * nStage_x_data[n - 1][18], J * nStage_x_data[n - 1][19], J * nStage_x_data[n - 1][20], J * nStage_x_data[n - 1][21]])])\n",
    "                new_threshold_h = np.full((1, 1), (1 - inputSize) * J)\n",
    "                new_threshold_h = np.hstack([n1Stage_threshold_h, new_threshold_h])\n",
    "                if nStage_y_data[n - 1] > 0:\n",
    "                    new_weights_o = n1Stage_b - n1Stage_nCase\n",
    "                    if new_weights_o == 0:\n",
    "                        new_weights_o = 0.1\n",
    "                else:\n",
    "                    new_weights_o = n1Stage_a - n1Stage_nCase\n",
    "                    if new_weights_o == 0: \n",
    "                        new_weights_o = -0.1\n",
    "                new_weights_o = np.vstack([n1Stage_weights_o, new_weights_o]) \n",
    "                sess.run(weights_h.assign(new_weights_h))\n",
    "                sess.run(weights_o.assign(new_weights_o))\n",
    "                sess.run(threshold_h.assign(new_threshold_h))\n",
    "                sess.run(threshold_o.assign(n1Stage_threshold_o))\n",
    "\n",
    "                pred_ = sess.run(outputLayer, feed_dict={xs: nStage_x_data})\n",
    "                a = min(pred_[a_data])\n",
    "                b = max(pred_[b_data])\n",
    "                if(a > b):\n",
    "                    print(\"   cramming succes\")\n",
    "                else:\n",
    "                    print(\"   cramming error, learning break\")\n",
    "                    ud = True                    \n",
    "                break\n",
    "    print(\"   learning_times:\", learning_times)\n",
    "    if ud: break\n",
    "\n",
    "    if hiddenNodesNum != 1:\n",
    "        #Step7: The Softening and Integrating mechanism\n",
    "        hiddenNodesIndex = 1\n",
    "        while hiddenNodesIndex <= hiddenNodesNum:\n",
    "            print(\"   Sof&Int: \", hiddenNodesIndex, \"/\", hiddenNodesNum)\n",
    "            #Softening\n",
    "            sftTimes = 0\n",
    "            lr = 0.1\n",
    "            pred_, temp_loss, temp_weights_h, temp_weights_o, temp_threshold_h, temp_threshold_o = sess.run([outputLayer, error, weights_h, weights_o, threshold_h, threshold_o], feed_dict={xs: nStage_x_data, ys: nStage_y_data})\n",
    "            a = min(pred_[a_data])\n",
    "            b = max(pred_[b_data])\n",
    "            while a > b and sftTimes < 500:\n",
    "                pre_weights_h, pre_weights_o, pre_threshold_h, pre_threshold_o = sess.run([weights_h, weights_o, threshold_h, threshold_o])\n",
    "                sess.run(train, feed_dict={xs: nStage_x_data, ys: nStage_y_data, learning_rate: lr})\n",
    "                pred_, loss = sess.run([outputLayer, error], feed_dict={xs: nStage_x_data, ys: nStage_y_data})             \n",
    "                if loss < temp_loss:\n",
    "                    lr = lr * 1.2\n",
    "                    a = min(pred_[a_data])\n",
    "                    b = max(pred_[b_data])\n",
    "                    temp_weights_h, temp_weights_o, temp_threshold_h, temp_threshold_o = sess.run([weights_h, weights_o, threshold_h, threshold_o])\n",
    "                    temp_loss = loss\n",
    "                    sftTimes += 1\n",
    "                    if sftTimes % 100 == 0:\n",
    "                        print(\"   S_loss: \", loss)\n",
    "                        print(\"   r: \", sess.run(regularizers))\n",
    "                        print(\"   hidd: \", sess.run(hiddenLayer, feed_dict={xs: nStage_x_data}))\n",
    "                        print(\"   weight_h: \", sess.run(weights_h))\n",
    "                else:\n",
    "                    if  lr > 0.05:\n",
    "                        sess.run(weights_h.assign(temp_weights_h))\n",
    "                        sess.run(weights_o.assign(temp_weights_o))\n",
    "                        sess.run(threshold_h.assign(temp_threshold_h))\n",
    "                        sess.run(threshold_o.assign(temp_threshold_o))\n",
    "                        lr = lr * 0.7\n",
    "                    else:\n",
    "                        break;     \n",
    "            sess.run(weights_h.assign(pre_weights_h))\n",
    "            sess.run(weights_o.assign(pre_weights_o))\n",
    "            sess.run(threshold_h.assign(pre_threshold_h))\n",
    "            sess.run(threshold_o.assign(pre_threshold_o))           \n",
    "            print(\"   Softening times:\", sftTimes)\n",
    "\n",
    "            pre_weights_h, pre_weights_o, pre_threshold_h, pre_threshold_o = sess.run([weights_h, weights_o, threshold_h, threshold_o])\n",
    "            #暫時忽略一個 hidden node\n",
    "            #error、init 需重新指派\n",
    "            hiddenNodesNum -= 1                 \n",
    "            hiddenLayer, weights_h, threshold_h = add_layer(xs, inputSize, hiddenNodesNum, tf.nn.relu)\n",
    "            outputLayer, weights_o, threshold_o = add_layer(hiddenLayer, hiddenNodesNum, 1)\n",
    "            error = tf.reduce_mean(tf.reduce_sum(tf.square(ys - outputLayer), reduction_indices=[1]))\n",
    "            #regularizers = tf.nn.l2_loss(weights_h) + tf.nn.l2_loss(weights_o) + tf.nn.l2_loss(threshold_h) + tf.nn.l2_loss(threshold_o)\n",
    "            regularizers = tf.math.multiply((0.001/(hiddenNodesNum + 1 + hiddenNodesNum * (inputSize + 1))), tf.reduce_sum(tf.square(weights_o))+tf.reduce_sum(tf.square(weights_h))+tf.reduce_sum(tf.square(threshold_h))+tf.reduce_sum(tf.square(threshold_o)))\n",
    "            error = error + regularizers\n",
    "            error2 = tf.square(ys - outputLayer)\n",
    "            train = tf.train.GradientDescentOptimizer(learning_rate).minimize(error)\n",
    "\n",
    "            init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "\n",
    "            sess.run(weights_h.assign(np.delete(pre_weights_h, hiddenNodesIndex - 1, 1)))\n",
    "            sess.run(weights_o.assign(np.delete(pre_weights_o, hiddenNodesIndex - 1, 0)))\n",
    "            sess.run(threshold_h.assign(np.delete(pre_threshold_h, hiddenNodesIndex - 1, 1)))\n",
    "            sess.run(threshold_o.assign(pre_threshold_o))\n",
    "\n",
    "            pred_, temp_loss, temp_weights_h, temp_weights_o, temp_threshold_h, temp_threshold_o = sess.run([outputLayer, error, weights_h, weights_o, threshold_h, threshold_o], feed_dict={xs: nStage_x_data, ys: nStage_y_data})\n",
    "            a = min(pred_[a_data])\n",
    "            b = max(pred_[b_data])\n",
    "            lr = 0.1\n",
    "            isIntegrating = True\n",
    "            iTimes = 0\n",
    "            while a <= b:\n",
    "                if iTimes >= 200:\n",
    "                    isIntegrating = False           \n",
    "                    break\n",
    "\n",
    "                sess.run(train, feed_dict={xs: nStage_x_data, ys: nStage_y_data, learning_rate: lr})\n",
    "                pred_, loss = sess.run([outputLayer, error], feed_dict={xs: nStage_x_data, ys: nStage_y_data})\n",
    "                if loss < temp_loss:\n",
    "                    lr = lr * 1.2\n",
    "                    a = min(pred_[a_data])\n",
    "                    b = max(pred_[b_data])\n",
    "                    temp_weights_h, temp_weights_o, temp_threshold_h, temp_threshold_o = sess.run([weights_h, weights_o, threshold_h, threshold_o])\n",
    "                    temp_loss = loss\n",
    "                    iTimes += 1\n",
    "                    if iTimes % 100 == 0:\n",
    "                        print(\"   I_loss: \", loss)\n",
    "                else:\n",
    "                    if  lr > 0.05:      \n",
    "                        sess.run(weights_h.assign(temp_weights_h))\n",
    "                        sess.run(weights_o.assign(temp_weights_o))\n",
    "                        sess.run(threshold_h.assign(temp_threshold_h))\n",
    "                        sess.run(threshold_o.assign(temp_threshold_o))\n",
    "                        lr = lr * 0.7\n",
    "                    else:\n",
    "                        isIntegrating = False           \n",
    "                        break\n",
    "            if isIntegrating: \n",
    "                print(\"   iTimes:\", iTimes, \", Integrating sucess\")\n",
    "            else:\n",
    "                #回復hidden node\n",
    "                hiddenNodesNum += 1                 \n",
    "                hiddenLayer, weights_h, threshold_h = add_layer(xs, inputSize, hiddenNodesNum, tf.nn.relu)\n",
    "                outputLayer, weights_o, threshold_o = add_layer(hiddenLayer, hiddenNodesNum, 1)\n",
    "                error = tf.reduce_mean(tf.reduce_sum(tf.square(ys - outputLayer), reduction_indices=[1]))\n",
    "                #regularizers = tf.nn.l2_loss(weights_h) + tf.nn.l2_loss(weights_o) + tf.nn.l2_loss(threshold_h) + tf.nn.l2_loss(threshold_o)\n",
    "                regularizers = tf.math.multiply((0.001/(hiddenNodesNum + 1 + hiddenNodesNum * (inputSize + 1))), tf.reduce_sum(tf.square(weights_o))+tf.reduce_sum(tf.square(weights_h))+tf.reduce_sum(tf.square(threshold_h))+tf.reduce_sum(tf.square(threshold_o)))\n",
    "                error = error + regularizers\n",
    "                error2 = tf.square(ys - outputLayer)\n",
    "                train = tf.train.GradientDescentOptimizer(learning_rate).minimize(error)\n",
    "\n",
    "                init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "                sess = tf.Session()\n",
    "                sess.run(init)\n",
    "\n",
    "                sess.run(weights_h.assign(pre_weights_h))\n",
    "                sess.run(weights_o.assign(pre_weights_o))\n",
    "                sess.run(threshold_h.assign(pre_threshold_h))\n",
    "                sess.run(threshold_o.assign(pre_threshold_o))\n",
    "                print(\"   iTimes:\", iTimes, \", Integrating fail, restore weight\")\n",
    "            hiddenNodesIndex += 1          \n",
    "    n+=1\n",
    "\n",
    "print(\"Complete!\")\n",
    "\n",
    "test_N = 174\n",
    "fp = open(\"test_x_data.txt\", \"r\", encoding = \"utf-8\")\n",
    "test_x_data = np.full((test_N, inputSize), 0.00)\n",
    "\n",
    "for i in range(test_N):\n",
    "    line = fp.readline()\n",
    "    for j in range(inputSize):\n",
    "        test_x_data[i][j] = np.asarray(line.split(), dtype=np.float64)[j]\n",
    "fp.close()\n",
    "\n",
    "fp = open(\"test_y_data.txt\", \"r\")\n",
    "line = fp.readline()\n",
    "test_y_data = []\n",
    "\n",
    "while line:\n",
    "    test_y_data.append(line.strip())\n",
    "    line = fp.readline()\n",
    "test_y_data = np.asarray(test_y_data, dtype=np.int32)\n",
    "test_y_data = np.reshape(test_y_data, (test_N, -1))\n",
    "\n",
    "pred_ = sess.run(outputLayer, feed_dict={xs: x_data})\n",
    "a = min(pred_[a_data])\n",
    "b = max(pred_[b_data])\n",
    "print(\"a: \", a)\n",
    "print(\"b: \", b)\n",
    "\n",
    "raw_pred = sess.run(outputLayer, feed_dict={xs: test_x_data})\n",
    "pred = []\n",
    "for i in range(test_N):        \n",
    "    if raw_pred[i] >= a:\n",
    "        pred.append(1)\n",
    "    elif raw_pred[i] <= b:\n",
    "        pred.append(-1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "pred = np.asarray(pred, dtype=np.int32)\n",
    "pred = np.reshape(pred, (test_N, -1))\n",
    "\n",
    "cpred = tf.equal(pred, test_y_data)\n",
    "acc = tf.reduce_mean(tf.cast(cpred, tf.float32))\n",
    "print(\"accuracy: \", sess.run(acc))\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
